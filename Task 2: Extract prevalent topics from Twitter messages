import tweepy
import re
import nltk.corpus
nltk.download('stopwords')
from nltk.corpus import stopwords
nltk.download('punkt')
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.decomposition import TruncatedSVD

API_key = 'NjjwbPkyMWIqZPUteAUT0W6Hz'
API_key_secret = 'UxaHXHr4DOaVSyaRGpUJ4T08PszNktWIjAZU5nTSTqV99AQJVl'
Bearer = 'AAAAAAAAAAAAAAAAAAAAAAmsfwEAAAAAJEayb5aUXjPZZdUVGtOPcPba9gs%3DBgRiQIz10Z8c0jXK7tvcEBe8djMbPvJCrfzkyc0xBHx1qkYmUy'
Access_Token = '1548787055454572544-k9v6yihv68p0mBfQJYorhAH5W3c7NC'
Access_Token_Secret = 'Q8FlaIe8yS9RY8zLzUQhpYleePduGVrqAPDvk8tOlIqgS'

client = tweepy.Client(bearer_token=Bearer)
query = "Wolverhampton -is:retweet"
response = client.search_recent_tweets(query=query, max_results=100)
retrieved_tweets = []
for tweet in response.data:
 retrieved_tweets.append(tweet.text)

# print(retrieved_tweets)

def cleantext(retrieved_tweets, i):
 retrieved_tweets[i] = retrieved_tweets[i].lower()  # converted the cases to low
 retrieved_tweets[i] = re.sub(r"(@\[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)|^rt|http.+?", "", retrieved_tweets[i])  # removed unicode characters
 retrieved_tweets[i] = ' '.join([word for word in retrieved_tweets[i].split() if word not in (stopwords.words('english'))])

list(map(lambda i: cleantext(retrieved_tweets, i), range(0, len(retrieved_tweets))))

# print(retrieved_tweets)

# TF-IDF + vectorize object
vectorizer = TfidfVectorizer(min_df=1)
model = vectorizer.fit_transform(retrieved_tweets)
data=pd.DataFrame(model.toarray(),columns=vectorizer.get_feature_names())

# print(data)
""" LDA Model
lda_model=LatentDirichletAllocation(n_components=5,learning_method='online',random_state=42,)
lda_top=lda_model.fit_transform(model)
print("Twitter Topics in Wolverhampton: ")
for i,topic in enumerate(lda_top[0]):
    print("Topic ",i,": ",topic*100,"%") """

LSA_model = TruncatedSVD(n_components=5, algorithm='randomized', n_iter=10)
lsa = LSA_model.fit_transform(model)
l=lsa[0]
print("Twitter Topics :")
for i,topic in enumerate(l):
    print("Topic ",i," : ",topic*100)
